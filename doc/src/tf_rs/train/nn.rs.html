<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="rustdoc">
    <meta name="description" content="Source to the Rust file `/home/nacho/develop/workspace/tf-rs/src/train/nn.rs`.">
    <meta name="keywords" content="rust, rustlang, rust-lang">

    <title>nn.rs.html -- source</title>

    <link rel="stylesheet" type="text/css" href="../../../normalize.css">
    <link rel="stylesheet" type="text/css" href="../../../rustdoc.css">
    <link rel="stylesheet" type="text/css" href="../../../main.css">
    

    
    
</head>
<body class="rustdoc source">
    <!--[if lte IE 8]>
    <div class="warning">
        This old browser is unsupported and will most likely display funky
        things.
    </div>
    <![endif]-->

    

    <nav class="sidebar">
        
        
    </nav>

    <nav class="sub">
        <form class="search-form js-only">
            <div class="search-container">
                <input class="search-input" name="search"
                       autocomplete="off"
                       placeholder="Click or press ‘S’ to search, ‘?’ for more options…"
                       type="search">
            </div>
        </form>
    </nav>

    <section id='main' class="content"><pre class="line-numbers"><span id="1">  1</span>
<span id="2">  2</span>
<span id="3">  3</span>
<span id="4">  4</span>
<span id="5">  5</span>
<span id="6">  6</span>
<span id="7">  7</span>
<span id="8">  8</span>
<span id="9">  9</span>
<span id="10"> 10</span>
<span id="11"> 11</span>
<span id="12"> 12</span>
<span id="13"> 13</span>
<span id="14"> 14</span>
<span id="15"> 15</span>
<span id="16"> 16</span>
<span id="17"> 17</span>
<span id="18"> 18</span>
<span id="19"> 19</span>
<span id="20"> 20</span>
<span id="21"> 21</span>
<span id="22"> 22</span>
<span id="23"> 23</span>
<span id="24"> 24</span>
<span id="25"> 25</span>
<span id="26"> 26</span>
<span id="27"> 27</span>
<span id="28"> 28</span>
<span id="29"> 29</span>
<span id="30"> 30</span>
<span id="31"> 31</span>
<span id="32"> 32</span>
<span id="33"> 33</span>
<span id="34"> 34</span>
<span id="35"> 35</span>
<span id="36"> 36</span>
<span id="37"> 37</span>
<span id="38"> 38</span>
<span id="39"> 39</span>
<span id="40"> 40</span>
<span id="41"> 41</span>
<span id="42"> 42</span>
<span id="43"> 43</span>
<span id="44"> 44</span>
<span id="45"> 45</span>
<span id="46"> 46</span>
<span id="47"> 47</span>
<span id="48"> 48</span>
<span id="49"> 49</span>
<span id="50"> 50</span>
<span id="51"> 51</span>
<span id="52"> 52</span>
<span id="53"> 53</span>
<span id="54"> 54</span>
<span id="55"> 55</span>
<span id="56"> 56</span>
<span id="57"> 57</span>
<span id="58"> 58</span>
<span id="59"> 59</span>
<span id="60"> 60</span>
<span id="61"> 61</span>
<span id="62"> 62</span>
<span id="63"> 63</span>
<span id="64"> 64</span>
<span id="65"> 65</span>
<span id="66"> 66</span>
<span id="67"> 67</span>
<span id="68"> 68</span>
<span id="69"> 69</span>
<span id="70"> 70</span>
<span id="71"> 71</span>
<span id="72"> 72</span>
<span id="73"> 73</span>
<span id="74"> 74</span>
<span id="75"> 75</span>
<span id="76"> 76</span>
<span id="77"> 77</span>
<span id="78"> 78</span>
<span id="79"> 79</span>
<span id="80"> 80</span>
<span id="81"> 81</span>
<span id="82"> 82</span>
<span id="83"> 83</span>
<span id="84"> 84</span>
<span id="85"> 85</span>
<span id="86"> 86</span>
<span id="87"> 87</span>
<span id="88"> 88</span>
<span id="89"> 89</span>
<span id="90"> 90</span>
<span id="91"> 91</span>
<span id="92"> 92</span>
<span id="93"> 93</span>
<span id="94"> 94</span>
<span id="95"> 95</span>
<span id="96"> 96</span>
<span id="97"> 97</span>
<span id="98"> 98</span>
<span id="99"> 99</span>
<span id="100">100</span>
<span id="101">101</span>
<span id="102">102</span>
<span id="103">103</span>
<span id="104">104</span>
<span id="105">105</span>
<span id="106">106</span>
<span id="107">107</span>
<span id="108">108</span>
<span id="109">109</span>
<span id="110">110</span>
<span id="111">111</span>
<span id="112">112</span>
<span id="113">113</span>
<span id="114">114</span>
<span id="115">115</span>
<span id="116">116</span>
<span id="117">117</span>
<span id="118">118</span>
<span id="119">119</span>
<span id="120">120</span>
<span id="121">121</span>
<span id="122">122</span>
<span id="123">123</span>
<span id="124">124</span>
<span id="125">125</span>
<span id="126">126</span>
<span id="127">127</span>
<span id="128">128</span>
<span id="129">129</span>
<span id="130">130</span>
<span id="131">131</span>
<span id="132">132</span>
<span id="133">133</span>
<span id="134">134</span>
<span id="135">135</span>
<span id="136">136</span>
<span id="137">137</span>
<span id="138">138</span>
<span id="139">139</span>
<span id="140">140</span>
<span id="141">141</span>
<span id="142">142</span>
<span id="143">143</span>
<span id="144">144</span>
<span id="145">145</span>
<span id="146">146</span>
<span id="147">147</span>
<span id="148">148</span>
<span id="149">149</span>
<span id="150">150</span>
<span id="151">151</span>
<span id="152">152</span>
<span id="153">153</span>
<span id="154">154</span>
<span id="155">155</span>
<span id="156">156</span>
<span id="157">157</span>
<span id="158">158</span>
<span id="159">159</span>
<span id="160">160</span>
<span id="161">161</span>
<span id="162">162</span>
<span id="163">163</span>
<span id="164">164</span>
<span id="165">165</span>
<span id="166">166</span>
<span id="167">167</span>
<span id="168">168</span>
<span id="169">169</span>
<span id="170">170</span>
<span id="171">171</span>
<span id="172">172</span>
<span id="173">173</span>
<span id="174">174</span>
<span id="175">175</span>
<span id="176">176</span>
<span id="177">177</span>
<span id="178">178</span>
<span id="179">179</span>
<span id="180">180</span>
<span id="181">181</span>
<span id="182">182</span>
<span id="183">183</span>
<span id="184">184</span>
<span id="185">185</span>
<span id="186">186</span>
<span id="187">187</span>
<span id="188">188</span>
<span id="189">189</span>
<span id="190">190</span>
<span id="191">191</span>
<span id="192">192</span>
<span id="193">193</span>
<span id="194">194</span>
<span id="195">195</span>
<span id="196">196</span>
<span id="197">197</span>
<span id="198">198</span>
<span id="199">199</span>
<span id="200">200</span>
<span id="201">201</span>
<span id="202">202</span>
<span id="203">203</span>
<span id="204">204</span>
<span id="205">205</span>
<span id="206">206</span>
<span id="207">207</span>
<span id="208">208</span>
<span id="209">209</span>
<span id="210">210</span>
<span id="211">211</span>
<span id="212">212</span>
<span id="213">213</span>
<span id="214">214</span>
<span id="215">215</span>
<span id="216">216</span>
<span id="217">217</span>
<span id="218">218</span>
<span id="219">219</span>
<span id="220">220</span>
<span id="221">221</span>
<span id="222">222</span>
<span id="223">223</span>
<span id="224">224</span>
<span id="225">225</span>
<span id="226">226</span>
<span id="227">227</span>
<span id="228">228</span>
<span id="229">229</span>
<span id="230">230</span>
<span id="231">231</span>
<span id="232">232</span>
<span id="233">233</span>
<span id="234">234</span>
<span id="235">235</span>
<span id="236">236</span>
<span id="237">237</span>
<span id="238">238</span>
<span id="239">239</span>
<span id="240">240</span>
<span id="241">241</span>
<span id="242">242</span>
<span id="243">243</span>
<span id="244">244</span>
<span id="245">245</span>
<span id="246">246</span>
<span id="247">247</span>
<span id="248">248</span>
<span id="249">249</span>
<span id="250">250</span>
<span id="251">251</span>
<span id="252">252</span>
<span id="253">253</span>
<span id="254">254</span>
<span id="255">255</span>
<span id="256">256</span>
<span id="257">257</span>
<span id="258">258</span>
<span id="259">259</span>
<span id="260">260</span>
<span id="261">261</span>
<span id="262">262</span>
<span id="263">263</span>
<span id="264">264</span>
<span id="265">265</span>
<span id="266">266</span>
<span id="267">267</span>
<span id="268">268</span>
<span id="269">269</span>
<span id="270">270</span>
<span id="271">271</span>
<span id="272">272</span>
<span id="273">273</span>
<span id="274">274</span>
<span id="275">275</span>
<span id="276">276</span>
<span id="277">277</span>
<span id="278">278</span>
<span id="279">279</span>
<span id="280">280</span>
<span id="281">281</span>
<span id="282">282</span>
<span id="283">283</span>
<span id="284">284</span>
<span id="285">285</span>
<span id="286">286</span>
<span id="287">287</span>
<span id="288">288</span>
<span id="289">289</span>
<span id="290">290</span>
<span id="291">291</span>
<span id="292">292</span>
<span id="293">293</span>
<span id="294">294</span>
<span id="295">295</span>
<span id="296">296</span>
<span id="297">297</span>
<span id="298">298</span>
<span id="299">299</span>
<span id="300">300</span>
<span id="301">301</span>
<span id="302">302</span>
<span id="303">303</span>
<span id="304">304</span>
<span id="305">305</span>
<span id="306">306</span>
<span id="307">307</span>
<span id="308">308</span>
<span id="309">309</span>
<span id="310">310</span>
<span id="311">311</span>
<span id="312">312</span>
<span id="313">313</span>
<span id="314">314</span>
<span id="315">315</span>
<span id="316">316</span>
<span id="317">317</span>
<span id="318">318</span>
<span id="319">319</span>
<span id="320">320</span>
<span id="321">321</span>
<span id="322">322</span>
<span id="323">323</span>
<span id="324">324</span>
<span id="325">325</span>
<span id="326">326</span>
</pre><pre class="rust ">
<span class="doccomment">//! Neural network support.</span>

<span class="kw">use</span> <span class="ident">std</span>::<span class="ident">path</span>::{<span class="ident">Path</span>, <span class="ident">PathBuf</span>};

<span class="kw">use</span> <span class="kw">super</span>::<span class="kw-2">*</span>;
<span class="kw">use</span> <span class="ident">ops</span>::{<span class="ident">ControlFlow</span>, <span class="ident">array_ops</span>, <span class="ident">math_ops</span>};

<span class="comment">/*
pub fn in_top_k&lt;C, Tx, Ty&gt;(context: &amp;mut C,
                            predictions: Tx,
                            targets: Ty,
                            k: u32)
                            -&gt; Result&lt;Tensor&gt;
    where Tx: Into&lt;Tensor&gt;,
            Ty: Into&lt;Tensor&gt;
{
}

pub fn l2_loss&lt;C, Tx&gt;(context: &amp;mut C, tensor: Tx) -&gt; Result&lt;Tensor&gt;
    where Tx: Into&lt;Tensor&gt;
{
}

pub fn sparse_softmax_cross_entropy_with_logits&lt;C, Tx, Ty&gt;(context: &amp;mut C,
                                                            tensor: Tx,
                                                            logits: Ty)
                                                            -&gt; Result&lt;Tensor&gt;
    where Tx: Into&lt;Tensor&gt;,
            Ty: Into&lt;Tensor&gt;
{
}
*/</span>

<span class="comment">///// BiasAdd /////</span>

<span class="doccomment">///  Adds `bias` to `value`.</span>
<span class="doccomment">///</span>
<span class="doccomment">///  This is (mostly) a special case of `tf.add` where `bias` is restricted to 1-D.</span>
<span class="doccomment">///  Broadcasting is supported, so `value` may have any number of dimensions.</span>
<span class="doccomment">///  Unlike `tf.add`, the type of `bias` is allowed to differ from `value` in the</span>
<span class="doccomment">///  case where both types are quantized.</span>
<span class="doccomment">///</span>
<span class="doccomment">///  Args:</span>
<span class="doccomment">///    value: A `Tensor` with type `float`, `double`, `int64`, `int32`, `uint8`,</span>
<span class="doccomment">///      `int16`, `int8`, `complex64`, or `complex128`.</span>
<span class="doccomment">///    bias: A 1-D `Tensor` with size matching the last dimension of `value`.</span>
<span class="doccomment">///      Must be the same type as `value` unless `value` is a quantized type,</span>
<span class="doccomment">///      in which case a different quantized type may be used.</span>
<span class="doccomment">///    data_format: A string. &#39;NHWC&#39; and &#39;NCHW&#39; are supported.</span>
<span class="doccomment">///    name: A name for the operation (optional).</span>
<span class="doccomment">///</span>
<span class="doccomment">///  Returns:</span>
<span class="doccomment">///    A `Tensor` with the same type as `value`.</span>
<span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">bias_add</span><span class="op">&lt;</span><span class="ident">Tx</span>, <span class="ident">B</span>, <span class="ident">S</span><span class="op">&gt;</span>(
    <span class="ident">context</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">Scope</span>,
    <span class="ident">value</span>: <span class="ident">Tx</span>,
    <span class="ident">bias</span>: <span class="ident">B</span>,
    <span class="ident">data_format</span>: <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">str</span><span class="op">&gt;</span>,
    <span class="ident">name</span>: <span class="ident">S</span>,
) <span class="op">-&gt;</span> <span class="prelude-ty">Result</span><span class="op">&lt;</span><span class="ident">Tensor</span><span class="op">&gt;</span>
<span class="kw">where</span>
    <span class="ident">Tx</span>: <span class="ident">Into</span><span class="op">&lt;</span><span class="ident">Tensor</span><span class="op">&gt;</span>,
    <span class="ident">B</span>: <span class="ident">Into</span><span class="op">&lt;</span><span class="ident">Tensor</span><span class="op">&gt;</span>,
    <span class="ident">S</span>: <span class="ident">AsRef</span><span class="op">&lt;</span><span class="ident">Path</span><span class="op">&gt;</span>,
{
    <span class="ident">context</span>.<span class="ident">name_scope</span>(<span class="ident">name</span>.<span class="ident">as_ref</span>(), <span class="prelude-val">Some</span>(<span class="string">&quot;BiasAdd&quot;</span>.<span class="ident">as_ref</span>()));
    <span class="kw">let</span> <span class="ident">value</span> <span class="op">=</span> <span class="ident">value</span>.<span class="ident">into</span>();
    <span class="kw">let</span> <span class="ident">bias</span> <span class="op">=</span> <span class="ident">bias</span>.<span class="ident">into</span>();
    <span class="kw">let</span> <span class="ident">d_id</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> [<span class="kw-2">&amp;</span><span class="ident">str</span>] <span class="op">=</span> <span class="kw-2">&amp;</span><span class="kw-2">mut</span> [<span class="string">&quot;&quot;</span>];
    <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">bias_add</span> <span class="op">=</span> <span class="ident">BiasAdd</span>::<span class="ident">new</span>(<span class="ident">value</span>, <span class="ident">bias</span>, <span class="ident">name</span>)<span class="question-mark">?</span>;
    <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">data_format</span>) <span class="op">=</span> <span class="ident">data_format</span> {
        <span class="ident">d_id</span>[<span class="number">0</span>] <span class="op">=</span> <span class="ident">validate_convnet_data_dormat</span>(<span class="ident">data_format</span>)<span class="question-mark">?</span>;
        <span class="ident">bias_add</span> <span class="op">=</span> <span class="ident">bias_add</span>.<span class="ident">data_format</span>(<span class="kw-2">&amp;</span><span class="ident">d_id</span>);
    }
    <span class="ident">context</span>.<span class="ident">install</span>(<span class="ident">bias_add</span>)
}

<span class="macro">add_new_op</span><span class="macro">!</span>(<span class="ident">BiasAdd</span>,
    <span class="ident">constructor</span>: [<span class="macro">add_new_op</span><span class="macro">!</span>(<span class="ident">BIN</span> <span class="ident">CONSTRUCTOR</span>: <span class="ident">BiasAdd</span>, <span class="ident">Init</span>: []);],
    <span class="ident">digest</span>: [<span class="ident">DEFAULT_DIGEST</span>: <span class="ident">BiasAdd</span>, <span class="ident">INPUT0</span>],
    <span class="ident">extra_funcs</span>: [
        <span class="kw">fn</span> <span class="ident">data_format</span>(<span class="kw-2">mut</span> <span class="self">self</span>, <span class="ident">val</span>: <span class="kw-2">&amp;</span><span class="lifetime">&#39;a</span> [<span class="kw-2">&amp;</span><span class="lifetime">&#39;a</span> <span class="ident">str</span>]) <span class="op">-&gt;</span> <span class="self">Self</span> {
            <span class="self">self</span>.<span class="ident">attributes</span>.<span class="ident">push</span>((<span class="string">&quot;data_format&quot;</span>, <span class="bool-val">false</span>, <span class="ident">Attribute</span>::<span class="ident">String</span>(<span class="ident">val</span>)));
            <span class="self">self</span>
        }
    ], 
    <span class="ident">extra_attr</span>: [],
    <span class="ident">output</span>: [<span class="ident">Tensor</span>],
);


<span class="comment">///// LogSoftmax /////</span>

<span class="doccomment">///  Computes log softmax activations.</span>
<span class="doccomment">///</span>
<span class="doccomment">///  For each batch `i` and class `j` we have</span>
<span class="doccomment">///      logsoftmax = logits - log(reduce_sum(exp(logits), dim))</span>
<span class="doccomment">///</span>
<span class="doccomment">///  Args:</span>
<span class="doccomment">///    logits: A non-empty `Tensor`. Must be one of the following types: `half`,</span>
<span class="doccomment">///      `float32`, `float64`.</span>
<span class="doccomment">///    dim: The dimension softmax would be performed on. The default is -1 which</span>
<span class="doccomment">///      indicates the last dimension.</span>
<span class="doccomment">///    name: A name for the operation (optional).</span>
<span class="doccomment">///</span>
<span class="doccomment">///  Returns:</span>
<span class="doccomment">///    A `Tensor`. Has the same type as `logits`. Same shape as `logits`.</span>
<span class="doccomment">///    Error if `logits` is empty or `dim` is beyond the last dimension of `logits`.</span>
<span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">log_softmax</span><span class="op">&lt;</span><span class="ident">L</span>, <span class="ident">S</span>, <span class="ident">TeS</span><span class="op">&gt;</span>(
    <span class="ident">context</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">Scope</span>,
    <span class="ident">logits</span>: <span class="ident">L</span>,
    <span class="ident">dim</span>: <span class="ident">TeS</span>,
    <span class="ident">name</span>: <span class="ident">S</span>,
) <span class="op">-&gt;</span> <span class="prelude-ty">Result</span><span class="op">&lt;</span><span class="ident">Tensor</span><span class="op">&gt;</span>
<span class="kw">where</span>
    <span class="ident">L</span>: <span class="ident">Into</span><span class="op">&lt;</span><span class="ident">Tensor</span><span class="op">&gt;</span>,
    <span class="ident">S</span>: <span class="ident">AsRef</span><span class="op">&lt;</span><span class="ident">Path</span><span class="op">&gt;</span>,
    <span class="ident">TeS</span>: <span class="ident">ShapeSize</span>,
{
    <span class="ident">softmax_helper</span>(<span class="ident">context</span>, <span class="ident">logits</span>.<span class="ident">into</span>(), <span class="bool-val">true</span>, <span class="ident">dim</span>.<span class="ident">as_i32</span>(), <span class="ident">name</span>.<span class="ident">as_ref</span>())
}

<span class="macro">add_new_op</span><span class="macro">!</span>(<span class="ident">LogSoftmax</span>, 
    <span class="ident">constructor</span>: [
        <span class="macro">add_new_op</span><span class="macro">!</span>(<span class="ident">UNARY</span> <span class="ident">CONSTRUCTOR</span>: <span class="ident">LogSoftmax</span>, <span class="ident">Init</span>: []);
    ],
    <span class="ident">digest</span>: [<span class="ident">DEFAULT_DIGEST</span>: <span class="ident">LogSoftmax</span>, <span class="ident">INPUT0</span>],
    <span class="ident">extra_funcs</span>: [], 
    <span class="ident">extra_attr</span>: [],
    <span class="ident">output</span>: [<span class="ident">Tensor</span>],
);


<span class="comment">///// Relu /////</span>

<span class="doccomment">///  Computes rectified linear: `max(features, 0)`.</span>
<span class="doccomment">///</span>
<span class="doccomment">///  Args:</span>
<span class="doccomment">///    features: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `half`.</span>
<span class="doccomment">///    name: A name for the operation (optional).</span>
<span class="doccomment">///</span>
<span class="doccomment">///  Returns:</span>
<span class="doccomment">///    A `Tensor`. Has the same type as `features`.</span>
<span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">relu</span><span class="op">&lt;</span><span class="ident">F</span>, <span class="ident">S</span><span class="op">&gt;</span>(<span class="ident">scope</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">Scope</span>, <span class="ident">features</span>: <span class="ident">F</span>, <span class="ident">name</span>: <span class="ident">S</span>) <span class="op">-&gt;</span> <span class="prelude-ty">Result</span><span class="op">&lt;</span><span class="ident">Tensor</span><span class="op">&gt;</span>
<span class="kw">where</span>
    <span class="ident">F</span>: <span class="ident">Into</span><span class="op">&lt;</span><span class="ident">Tensor</span><span class="op">&gt;</span>,
    <span class="ident">S</span>: <span class="ident">AsRef</span><span class="op">&lt;</span><span class="ident">Path</span><span class="op">&gt;</span>,
{
    <span class="ident">scope</span>.<span class="ident">install</span>(<span class="ident">Relu</span>::<span class="ident">new</span>(<span class="ident">features</span>.<span class="ident">into</span>(), <span class="ident">name</span>)<span class="question-mark">?</span>)
}

<span class="macro">add_new_op</span><span class="macro">!</span>(<span class="ident">Relu</span>, 
    <span class="ident">constructor</span>: [
        <span class="macro">add_new_op</span><span class="macro">!</span>(<span class="ident">UNARY</span> <span class="ident">CONSTRUCTOR</span>: <span class="ident">Relu</span>, <span class="ident">Init</span>: []);
    ],
    <span class="ident">digest</span>: [<span class="ident">DEFAULT_DIGEST</span>: <span class="ident">Relu</span>, <span class="ident">INPUT0</span>],
    <span class="ident">extra_funcs</span>: [], 
    <span class="ident">extra_attr</span>: [],
    <span class="ident">output</span>: [<span class="ident">Tensor</span>],
);


<span class="comment">///// Softmax /////</span>

<span class="doccomment">///  Computes softmax activations.</span>
<span class="doccomment">///</span>
<span class="doccomment">///  For each batch `i` and class `j` we have</span>
<span class="doccomment">///      softmax = exp(logits) / reduce_sum(exp(logits), dim)</span>
<span class="doccomment">///</span>
<span class="doccomment">///  Args:</span>
<span class="doccomment">///    logits: A non-empty `Tensor`. Must be one of the following types: `half`,</span>
<span class="doccomment">///      `float32`, `float64`.</span>
<span class="doccomment">///    dim: The dimension softmax would be performed on. The default is -1 which</span>
<span class="doccomment">///      indicates the last dimension.</span>
<span class="doccomment">///    name: A name for the operation (optional).</span>
<span class="doccomment">///</span>
<span class="doccomment">///  Returns:</span>
<span class="doccomment">///    A `Tensor`. Has the same type as `logits`. Same shape as `logits`.</span>
<span class="doccomment">///    Error: if `logits` is empty or `dim` is beyond the last dimension of `logits`.</span>
<span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">softmax</span><span class="op">&lt;</span><span class="ident">L</span>, <span class="ident">S</span>, <span class="ident">TeS</span><span class="op">&gt;</span>(
    <span class="ident">context</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">Scope</span>,
    <span class="ident">logits</span>: <span class="ident">L</span>,
    <span class="ident">dim</span>: <span class="ident">TeS</span>,
    <span class="ident">name</span>: <span class="ident">S</span>,
) <span class="op">-&gt;</span> <span class="prelude-ty">Result</span><span class="op">&lt;</span><span class="ident">Tensor</span><span class="op">&gt;</span>
<span class="kw">where</span>
    <span class="ident">L</span>: <span class="ident">Into</span><span class="op">&lt;</span><span class="ident">Tensor</span><span class="op">&gt;</span>,
    <span class="ident">S</span>: <span class="ident">AsRef</span><span class="op">&lt;</span><span class="ident">Path</span><span class="op">&gt;</span>,
    <span class="ident">TeS</span>: <span class="ident">ShapeSize</span>,
{
    <span class="ident">softmax_helper</span>(<span class="ident">context</span>, <span class="ident">logits</span>.<span class="ident">into</span>(), <span class="bool-val">false</span>, <span class="ident">dim</span>.<span class="ident">as_i32</span>(), <span class="ident">name</span>.<span class="ident">as_ref</span>())
}

<span class="macro">add_new_op</span><span class="macro">!</span>(<span class="ident">Softmax</span>, 
    <span class="ident">constructor</span>: [
        <span class="macro">add_new_op</span><span class="macro">!</span>(<span class="ident">UNARY</span> <span class="ident">CONSTRUCTOR</span>: <span class="ident">Softmax</span>, <span class="ident">Init</span>: []);
    ],
    <span class="ident">digest</span>: [<span class="ident">DEFAULT_DIGEST</span>: <span class="ident">Softmax</span>, <span class="ident">INPUT0</span>],
    <span class="ident">extra_funcs</span>: [], 
    <span class="ident">extra_attr</span>: [],
    <span class="ident">output</span>: [<span class="ident">Tensor</span>],
);


<span class="doccomment">///  Helper function for softmax and log_softmax.</span>
<span class="doccomment">///</span>
<span class="doccomment">///  It reshapes and transposes the input logits into a 2-D Tensor and then invokes</span>
<span class="doccomment">///  the tf.nn._softmax or tf.nn._log_softmax function. The output would be</span>
<span class="doccomment">///  transposed and reshaped back.</span>
<span class="doccomment">///</span>
<span class="doccomment">///  Args:</span>
<span class="doccomment">///    logits: A non-empty `Tensor`. Must be one of the following types: `half`,</span>
<span class="doccomment">///      `float32`, `float64`.</span>
<span class="doccomment">///    compute_op: Either gen_nn_ops._softmax or gen_nn_ops._log_softmax</span>
<span class="doccomment">///    dim: The dimension softmax would be performed on. The default is -1 which</span>
<span class="doccomment">///      indicates the last dimension.</span>
<span class="doccomment">///    name: A name for the operation (optional).</span>
<span class="doccomment">///</span>
<span class="doccomment">///  Returns:</span>
<span class="doccomment">///    A `Tensor`. Has the same type as `logits`. Same shape as `logits`.</span>
<span class="doccomment">///    Error if `logits` is empty or `dim` is beyond the last</span>
<span class="doccomment">///      dimension of `logits`.</span>
<span class="kw">fn</span> <span class="ident">softmax_helper</span>(
    <span class="ident">context</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">Scope</span>,
    <span class="kw-2">mut</span> <span class="ident">logits</span>: <span class="ident">Tensor</span>,
    <span class="ident">is_log_softmax</span>: <span class="ident">bool</span>,
    <span class="ident">dim</span>: <span class="ident">i32</span>,
    <span class="ident">name</span>: <span class="kw-2">&amp;</span><span class="ident">Path</span>,
) <span class="op">-&gt;</span> <span class="prelude-ty">Result</span><span class="op">&lt;</span><span class="ident">Tensor</span><span class="op">&gt;</span> {

    <span class="kw">fn</span> <span class="ident">swap_axis</span>(
        <span class="ident">scope</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">Scope</span>,
        <span class="ident">logits</span>: <span class="ident">Tensor</span>,
        <span class="ident">dim_index</span>: <span class="ident">i32</span>,
        <span class="ident">last_index</span>: <span class="ident">Tensor</span>,
        <span class="ident">name</span>: <span class="kw-2">&amp;</span><span class="ident">Path</span>,
    ) <span class="op">-&gt;</span> <span class="prelude-ty">Result</span><span class="op">&lt;</span><span class="ident">Tensor</span><span class="op">&gt;</span> {
        <span class="kw">let</span> <span class="ident">r0</span> <span class="op">=</span> <span class="ident">ops</span>::<span class="ident">range</span>(<span class="ident">scope</span>, <span class="number">0_i32</span>, <span class="ident">dim_index</span>, <span class="number">1_i32</span>, <span class="ident">name</span>)<span class="question-mark">?</span>;
        <span class="kw">let</span> <span class="ident">r1</span> <span class="op">=</span> <span class="ident">ops</span>::<span class="ident">range</span>(<span class="ident">scope</span>, <span class="number">0_i32</span>, <span class="ident">dim_index</span> <span class="op">+</span> <span class="number">1</span>, <span class="number">1_i32</span>, <span class="ident">name</span>)<span class="question-mark">?</span>;
        <span class="kw">let</span> <span class="ident">r2</span> <span class="op">=</span> <span class="ident">Constant</span>::<span class="ident">new</span>(<span class="ident">scope</span>, <span class="kw-2">&amp;</span>[<span class="ident">dim_index</span>], <span class="kw-2">&amp;</span>[] <span class="kw">as</span> <span class="kw-2">&amp;</span>[<span class="ident">i32</span>]).<span class="ident">into</span>();
        <span class="kw">let</span> <span class="ident">c</span> <span class="op">=</span> <span class="ident">array_ops</span>::<span class="ident">concat</span>(<span class="ident">scope</span>, <span class="macro">vec</span><span class="macro">!</span>[<span class="ident">r0</span>, <span class="ident">last_index</span>, <span class="ident">r1</span>, <span class="ident">r2</span>], <span class="number">0</span>, <span class="string">&quot;&quot;</span>)<span class="question-mark">?</span>;
        <span class="ident">array_ops</span>::<span class="ident">transpose</span>(<span class="ident">scope</span>, <span class="ident">logits</span>, <span class="prelude-val">Some</span>(<span class="ident">c</span>), <span class="ident">name</span>)
    }

    <span class="comment">// We need its original shape for shape inference.</span>
    <span class="kw">let</span> <span class="ident">shape</span> <span class="op">=</span> <span class="ident">logits</span>.<span class="ident">get_shape</span>(<span class="ident">context</span>);
    <span class="kw">let</span> <span class="ident">ndims</span> <span class="op">=</span> <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">n</span>) <span class="op">=</span> <span class="ident">shape</span>.<span class="ident">dims</span>() {
        <span class="ident">n</span> <span class="kw">as</span> <span class="ident">i32</span>
    } <span class="kw">else</span> {
        <span class="kw">return</span> <span class="prelude-val">Err</span>(<span class="ident">Error</span>::<span class="ident">from</span>(<span class="string">&quot;shape of logits tensor must be defined for softmax operation.&quot;</span>));
    };
    <span class="kw">let</span> <span class="ident">is_last_dim</span> <span class="op">=</span> <span class="ident">dim</span> <span class="op">==</span> <span class="op">-</span><span class="number">1</span> <span class="op">||</span> <span class="ident">dim</span> <span class="op">==</span> <span class="ident">ndims</span> <span class="op">-</span> <span class="number">1</span>;
    <span class="kw">if</span> (<span class="ident">ndims</span> <span class="op">==</span> <span class="number">2</span>) <span class="op">&amp;&amp;</span> <span class="ident">is_last_dim</span> {
        <span class="kw">if</span> <span class="ident">is_log_softmax</span> {
            <span class="kw">return</span> <span class="ident">context</span>.<span class="ident">install</span>(<span class="ident">LogSoftmax</span>::<span class="ident">new</span>(<span class="ident">logits</span>, <span class="ident">name</span>)<span class="question-mark">?</span>);
        } <span class="kw">else</span> {
            <span class="kw">return</span> <span class="ident">context</span>.<span class="ident">install</span>(<span class="ident">Softmax</span>::<span class="ident">new</span>(<span class="ident">logits</span>, <span class="ident">name</span>)<span class="question-mark">?</span>);
        }
    }

    <span class="comment">// If dim is the last dimension, simply reshape the logits to a matrix and</span>
    <span class="comment">// apply the internal softmax.</span>

    <span class="comment">// Swap logits&#39; dimension of dim and its last dimension.</span>
    <span class="kw">let</span> <span class="ident">input_rank</span> <span class="op">=</span> <span class="ident">array_ops</span>::<span class="ident">rank</span>(<span class="ident">context</span>, <span class="ident">logits</span>, <span class="string">&quot;&quot;</span>)<span class="question-mark">?</span>;

    <span class="kw">let</span> <span class="ident">s</span> <span class="op">=</span> {
        <span class="kw">let</span> <span class="ident">n</span> <span class="op">=</span> <span class="ident">context</span>.<span class="ident">constant</span>(<span class="kw-2">&amp;</span>[<span class="number">1</span>], <span class="kw-2">&amp;</span>[] <span class="kw">as</span> <span class="kw-2">&amp;</span>[<span class="ident">i32</span>], <span class="string">&quot;&quot;</span>)<span class="question-mark">?</span>;
        <span class="ident">ops</span>::<span class="ident">math_ops</span>::<span class="ident">sub</span>(<span class="ident">context</span>, <span class="ident">input_rank</span>, <span class="ident">n</span>, <span class="string">&quot;&quot;</span>)<span class="question-mark">?</span>
    };
    <span class="ident">logits</span> <span class="op">=</span> <span class="ident">swap_axis</span>(<span class="ident">context</span>, <span class="ident">logits</span>, <span class="ident">dim</span>, <span class="ident">s</span>, <span class="string">&quot;&quot;</span>.<span class="ident">as_ref</span>())<span class="question-mark">?</span>;
    <span class="kw">let</span> <span class="ident">shape_after_swap</span> <span class="op">=</span> <span class="ident">array_ops</span>::<span class="ident">shape</span>(<span class="ident">context</span>, <span class="ident">logits</span>, <span class="prelude-val">None</span>, <span class="string">&quot;&quot;</span>)<span class="question-mark">?</span>;

    <span class="comment">// Reshape logits into a matrix.</span>
    <span class="ident">logits</span> <span class="op">=</span> <span class="ident">flatten_outer_dims</span>(<span class="ident">context</span>, <span class="ident">logits</span>)<span class="question-mark">?</span>;

    <span class="comment">// Do the actual softmax on its last dimension.</span>
    <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">output</span> <span class="op">=</span> <span class="kw">if</span> <span class="ident">is_log_softmax</span> {
        <span class="ident">context</span>.<span class="ident">install</span>(<span class="ident">LogSoftmax</span>::<span class="ident">new</span>(<span class="ident">logits</span>, <span class="ident">name</span>)<span class="question-mark">?</span>)<span class="question-mark">?</span>
    } <span class="kw">else</span> {
        <span class="ident">context</span>.<span class="ident">install</span>(<span class="ident">Softmax</span>::<span class="ident">new</span>(<span class="ident">logits</span>, <span class="ident">name</span>)<span class="question-mark">?</span>)<span class="question-mark">?</span>
    };

    <span class="comment">// Transform back the output tensor.</span>
    <span class="ident">output</span> <span class="op">=</span> <span class="ident">array_ops</span>::<span class="ident">reshape</span>(<span class="ident">context</span>, <span class="ident">output</span>, <span class="ident">shape_after_swap</span>, <span class="string">&quot;&quot;</span>)<span class="question-mark">?</span>;
    <span class="ident">output</span> <span class="op">=</span> <span class="ident">swap_axis</span>(<span class="ident">context</span>, <span class="ident">output</span>, <span class="ident">dim</span>, <span class="ident">s</span>, <span class="ident">name</span>)<span class="question-mark">?</span>;

    <span class="comment">// Make shape inference work since reshape and transpose may erase its static shape.</span>
    <span class="ident">output</span> <span class="op">=</span> <span class="ident">output</span>.<span class="ident">set_shape</span>(<span class="ident">context</span>, <span class="ident">shape</span>)<span class="question-mark">?</span>;
    <span class="prelude-val">Ok</span>(<span class="ident">output</span>)
}

<span class="doccomment">/// Flattens logits&#39; outer dimensions and keep its last dimension.</span>
<span class="kw">fn</span> <span class="ident">flatten_outer_dims</span>(<span class="ident">scope</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">Scope</span>, <span class="ident">logits</span>: <span class="ident">Tensor</span>) <span class="op">-&gt;</span> <span class="prelude-ty">Result</span><span class="op">&lt;</span><span class="ident">Tensor</span><span class="op">&gt;</span> {
    <span class="kw">let</span> <span class="ident">r</span> <span class="op">=</span> <span class="ident">array_ops</span>::<span class="ident">rank</span>(<span class="ident">scope</span>, <span class="ident">logits</span>, <span class="string">&quot;&quot;</span>)<span class="question-mark">?</span>;
    <span class="kw">let</span> <span class="ident">last_dim_size</span> <span class="op">=</span> {
        <span class="kw">let</span> <span class="ident">s0</span> <span class="op">=</span> <span class="ident">array_ops</span>::<span class="ident">shape</span>(<span class="ident">scope</span>, <span class="ident">logits</span>, <span class="prelude-val">None</span>, <span class="string">&quot;&quot;</span>)<span class="question-mark">?</span>;
        <span class="kw">let</span> <span class="ident">s1</span> <span class="op">=</span> <span class="ident">math_ops</span>::<span class="ident">sub</span>(<span class="ident">scope</span>, <span class="ident">r</span>, <span class="number">1_i32</span>, <span class="string">&quot;&quot;</span>)<span class="question-mark">?</span>;
        <span class="ident">array_ops</span>::<span class="ident">slice</span>(<span class="ident">scope</span>, <span class="ident">s0</span>, <span class="ident">s1</span>, <span class="number">1_i32</span>, <span class="string">&quot;&quot;</span>)<span class="question-mark">?</span>
    };
    <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">output</span> <span class="op">=</span> {
        <span class="kw">let</span> <span class="ident">c0</span> <span class="op">=</span> <span class="ident">Constant</span>::<span class="ident">new</span>(<span class="ident">scope</span>, <span class="kw-2">&amp;</span>[<span class="number">1_i32</span>], <span class="kw-2">&amp;</span>[] <span class="kw">as</span> <span class="kw-2">&amp;</span>[<span class="ident">i32</span>]).<span class="ident">into</span>();
        <span class="kw">let</span> <span class="ident">c</span> <span class="op">=</span> <span class="ident">array_ops</span>::<span class="ident">concat</span>(<span class="ident">scope</span>, <span class="macro">vec</span><span class="macro">!</span>[<span class="ident">c0</span>, <span class="ident">last_dim_size</span>], <span class="number">0</span>, <span class="string">&quot;&quot;</span>)<span class="question-mark">?</span>;
        <span class="ident">array_ops</span>::<span class="ident">reshape</span>(<span class="ident">scope</span>, <span class="ident">logits</span>, <span class="ident">c</span>, <span class="string">&quot;&quot;</span>)<span class="question-mark">?</span>
    };

    <span class="comment">// Set output shaoe if known.</span>
    <span class="kw">let</span> <span class="ident">shape</span>: <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">i64</span><span class="op">&gt;&gt;</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">logits</span>.<span class="ident">get_shape</span>(<span class="ident">scope</span>).<span class="ident">into</span>();
    <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">shape</span>) <span class="op">=</span> <span class="ident">shape</span> {
        <span class="comment">//let shape.</span>
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">product</span> <span class="op">=</span> <span class="number">1</span>;
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">product_valid</span> <span class="op">=</span> <span class="bool-val">true</span>;
        <span class="kw">for</span> <span class="ident">d</span> <span class="kw">in</span> <span class="kw-2">&amp;</span><span class="ident">shape</span>[..<span class="ident">shape</span>.<span class="ident">len</span>()] {
            <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">d</span>) <span class="op">=</span> <span class="kw-2">*</span><span class="ident">d</span> {
                <span class="ident">product</span> <span class="op">*=</span> <span class="ident">d</span>;
            } <span class="kw">else</span> {
                <span class="ident">product_valid</span> <span class="op">=</span> <span class="bool-val">false</span>;
            }
        }
        <span class="kw">if</span> <span class="ident">product_valid</span> {
            <span class="kw">let</span> <span class="ident">output_shape</span> <span class="op">=</span> [<span class="ident">product</span>, <span class="ident">shape</span>.<span class="ident">last</span>().<span class="ident">unwrap</span>().<span class="ident">unwrap</span>()];
            <span class="ident">output</span> <span class="op">=</span> <span class="ident">array_ops</span>::<span class="ident">reshape</span>(<span class="ident">scope</span>, <span class="ident">output</span>, <span class="kw-2">&amp;</span><span class="ident">output_shape</span> <span class="kw">as</span> <span class="kw-2">&amp;</span>[<span class="ident">i64</span>], <span class="string">&quot;&quot;</span>)<span class="question-mark">?</span>;
        }
    }
    <span class="prelude-val">Ok</span>(<span class="ident">output</span>)
}
</pre>
</section>
    <section id='search' class="content hidden"></section>

    <section class="footer"></section>

    <aside id="help" class="hidden">
        <div>
            <h1 class="hidden">Help</h1>

            <div class="shortcuts">
                <h2>Keyboard Shortcuts</h2>

                <dl>
                    <dt>?</dt>
                    <dd>Show this help dialog</dd>
                    <dt>S</dt>
                    <dd>Focus the search field</dd>
                    <dt>↑</dt>
                    <dd>Move up in search results</dd>
                    <dt>↓</dt>
                    <dd>Move down in search results</dd>
                    <dt>&#9166;</dt>
                    <dd>Go to active search result</dd>
                    <dt>+</dt>
                    <dd>Collapse/expand all sections</dd>
                </dl>
            </div>

            <div class="infos">
                <h2>Search Tricks</h2>

                <p>
                    Prefix searches with a type followed by a colon (e.g.
                    <code>fn:</code>) to restrict the search to a given type.
                </p>

                <p>
                    Accepted types are: <code>fn</code>, <code>mod</code>,
                    <code>struct</code>, <code>enum</code>,
                    <code>trait</code>, <code>type</code>, <code>macro</code>,
                    and <code>const</code>.
                </p>

                <p>
                    Search functions by type signature (e.g.
                    <code>vec -> usize</code> or <code>* -> vec</code>)
                </p>
            </div>
        </div>
    </aside>

    

    <script>
        window.rootPath = "../../../";
        window.currentCrate = "tf_rs";
    </script>
    <script src="../../../main.js"></script>
    <script defer src="../../../search-index.js"></script>
</body>
</html>